+ residues, normalisation, ReLU (not Heavyside), embeddings,
running on multiple GPUs, 1-bit MLP layer, the opaqueness and the attempts at
interpretability, language (and whether grammar is a thing), Whorf,
the universality of language, French having fewer verbs but more preposition.

+ system 1 and system 2, experiments that show brain activity
before we have consciously decided to do something,
thousand brain theory

gradient descent, the difficulties of alignment,
greedy vs tree of thought, Q*, music AI, neuron clock rate
and how the brain is therefore wider and shallower than GPT-3

+ overhang, fast takeoff, how scifi is usually wrong about humans having
more intuition than AIs

+ Σ(i³) = (Σi)², intermediate value theorem, godel, quantum mechanics,
proofs of pythagoras' theorem, obviousness, why some people don't like maths.

+ stats, OJ Simpson example, Monty Hall

+ ramanujan AI

### Things for next time

+ Computational irreducibility

+ Getting enough high-quality training data (including using synthetic data)

+ the quadratic cost of the attention layer

+ LoRA

+ why training doesn't get too stuck in a local minimum

+ GPT learnt in a cave (text only) and may know that dogs are
hairy without knowing what dogs or hairiness are.

+ othello

+ our small working memory, but our ability to integrate new information
(e.g. during a long conversation).
